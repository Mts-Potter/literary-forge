# Literary Forge

KI-gestÃ¼tztes Training fÃ¼r stilistische Mimesis durch Spaced Repetition und linguistische Analyse.

## ğŸ¯ Ãœberblick

Literary Forge ist eine vollstÃ¤ndig serverlose Web-Anwendung, die es Nutzern ermÃ¶glicht, die Schreibstile groÃŸer Autoren zu erlernen. Durch eine Kombination aus:

- **Spaced Repetition (SM-2 Algorithmus)** fÃ¼r langfristiges Lernen
- **Clientseitige NLP-Analyse** (UDPipe, Transformers.js)
- **KI-Feedback** via Claude 3.5 Haiku
- **0â‚¬ Fixkosten** dank Vercel & Supabase Free Tier

## ğŸ—ï¸ Architektur

### Tech Stack
- **Frontend**: Next.js 15 (App Router), TypeScript, Tailwind CSS
- **Backend**: Supabase (PostgreSQL + pgvector + Auth)
- **LLM**: Claude 3.5 Haiku via **AWS Bedrock** (nicht direkte Anthropic API)
- **NLP**: Mock-Implementierung (spÃ¤ter: UDPipe WASM + Transformers.js)
- **State Management**: Zustand mit localStorage-Persistenz

### Besonderheiten
- **Edge Runtime** fÃ¼r API-Routes (maximale Performance)
- **Rate Limiting** Ã¼ber PostgreSQL (Token-Bucket-Algorithmus)
- **Client-Side Computing** fÃ¼r NLP (keine Serverkosten)
- **Local-First** Datenarchitektur (IndexedDB/localStorage)

## ğŸ“‹ Setup-Anleitung

### 1. Supabase Projekt erstellen

1. Gehe zu [supabase.com](https://supabase.com) und erstelle ein kostenloses Konto
2. Erstelle ein neues Projekt:
   - **Name**: `literary-forge`
   - **Database Password**: Generiere ein sicheres Passwort (speichern!)
   - **Region**: Europe West (Frankfurt)
3. Warte ca. 2 Minuten bis das Projekt bereit ist

### 2. pgvector Extension aktivieren

1. Gehe im Supabase Dashboard zu: **Database â†’ Extensions**
2. Suche nach "vector"
3. Klicke auf **Enable** bei `vector`

### 3. Datenbank-Schema erstellen

1. Gehe zu: **SQL Editor**
2. Ã–ffne die Datei `supabase/migrations/001_initial_schema.sql` in deinem Projekt
3. Kopiere den kompletten SQL-Code
4. FÃ¼ge ihn im SQL Editor ein und klicke **RUN**

Das Schema erstellt:
- `source_texts` - Die Lerntexte mit Embeddings
- `user_progress` - SRS-Status fÃ¼r jeden Nutzer
- `user_quotas` / `ip_quotas` - Rate Limiting
- `check_and_consume_quota()` - RPC-Funktion fÃ¼r sichere Quotenverwaltung

### 4. API-Credentials extrahieren

1. Gehe zu: **Settings â†’ API**
2. Kopiere folgende Werte:
   - **Project URL** â†’ `NEXT_PUBLIC_SUPABASE_URL`
   - **anon public** â†’ `NEXT_PUBLIC_SUPABASE_PUBLISHABLE_KEY`
   - **service_role** â†’ `SUPABASE_SECRET_KEY` âš ï¸ GEHEIM!

### 5. AWS Bedrock Credentials (statt direkter Anthropic API)

âš ï¸ **Wichtig**: Dieses Projekt nutzt AWS Bedrock statt der direkten Anthropic API!

1. Stelle sicher, dass du AWS Bedrock Zugriff auf Claude 3.5 Haiku hast
2. Besorge deine AWS IAM Credentials (Access Key ID + Secret Access Key)
3. WÃ¤hle die Region (empfohlen: `us-east-1`)

### 6. Umgebungsvariablen konfigurieren

Ã–ffne `.env.local` und fÃ¼lle alle Werte aus:

```env
# Supabase Configuration
NEXT_PUBLIC_SUPABASE_URL=https://xxx.supabase.co
NEXT_PUBLIC_SUPABASE_PUBLISHABLE_KEY=eyJhbGc...
SUPABASE_SECRET_KEY=eyJhbGc...

# AWS Bedrock Configuration
AWS_ACCESS_KEY_ID=AKIA...
AWS_SECRET_ACCESS_KEY=...
AWS_REGION=us-east-1

# Bedrock Modell-ID fÃ¼r Claude 3.5 Haiku
BEDROCK_MODEL_ID=us.anthropic.claude-3-5-haiku-20241022-v1:0

# Rate Limiting Configuration
MAX_DAILY_ANALYSES_PER_USER=5
```

### 7. Development Server starten

```bash
npm run dev
```

Ã–ffne [http://localhost:3000](http://localhost:3000)

### 8. Content Import (Optional)

Um BÃ¼cher in die Datenbank zu importieren:

1. Navigiere zu `/login` und erstelle einen Account
2. Gehe zu `/admin/ingest` (oder klicke "ğŸ“š Admin Import" auf der Startseite)
3. WÃ¤hle eine `.txt` Datei (siehe `test-books/` fÃ¼r Beispiele)
4. FÃ¼lle Titel und Autor aus
5. Klicke "Analysieren" und dann "Speichern"

**Detaillierte Anleitung**: Siehe [BOOK_IMPORT_GUIDE.md](BOOK_IMPORT_GUIDE.md)

**Wichtig**: Das Import-System nutzt dieselbe NLP-Pipeline (UDPipe + Transformers.js) wie das Training, um wissenschaftliche Konsistenz der Stilmetriken zu garantieren.

## ğŸ“ Projektstruktur

```
literary-forge/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ page.tsx                    # Landing Page
â”‚   â”œâ”€â”€ layout.tsx                  # Root Layout
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ analyze/route.ts        # LLM Edge Function
â”œâ”€â”€ components/
â”‚   â””â”€â”€ editor/
â”‚       â””â”€â”€ ZenEditor.tsx           # Typewriter-Editor
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ supabase/
â”‚   â”‚   â”œâ”€â”€ client.ts               # Browser Supabase Client
â”‚   â”‚   â”œâ”€â”€ server.ts               # Server Supabase Client
â”‚   â”‚   â””â”€â”€ types.ts                # TypeScript Types
â”‚   â”œâ”€â”€ nlp/
â”‚   â”‚   â”œâ”€â”€ parsing.ts              # UDPipe Parser (Mock)
â”‚   â”‚   â”œâ”€â”€ embeddings.ts           # Embedding Generator (Mock)
â”‚   â”‚   â””â”€â”€ metrics.ts              # Stylometrische Berechnungen
â”‚   â””â”€â”€ srs/
â”‚       â””â”€â”€ sm2.ts                  # SM-2 Algorithmus
â””â”€â”€ supabase/
    â””â”€â”€ migrations/
        â””â”€â”€ 001_initial_schema.sql  # Datenbank-Schema
```

## ğŸ”’ Sicherheit

- **Row Level Security (RLS)** auf allen Tabellen
- **Rate Limiting** via PostgreSQL mit `SECURITY DEFINER` und fixed `search_path`
- **Edge-Only API Calls** - kein direkter Client-Zugriff auf RPC
- **IP-Extraktion** mit CSV-Handling fÃ¼r `x-forwarded-for`

## ğŸ’° Kostenstruktur (Free Tier)

### Limits
- **Vercel**: 100 GB Bandwidth, 100 GB-Hours Edge
- **Supabase**: 500 MB Datenbank, 2 GB Egress/Monat
- **Anthropic**: Pay-as-you-go

### Bei 100 aktiven Usern/Tag
- **Vercel**: 0â‚¬ (innerhalb Free Tier)
- **Supabase**: 0â‚¬ (Local-First hÃ¤lt DB klein)
- **AWS Bedrock**: ~10-15â‚¬/Monat (5 Analysen/User Ã— Claude 3.5 Haiku Preise auf Bedrock)

**Skalierung**: Serverkosten wachsen nicht mit Nutzern, da NLP-Arbeit auf Clients lÃ¤uft!

## ğŸš€ Deployment

### Vercel (empfohlen)

```bash
# Mit Vercel CLI
vercel

# Oder via GitHub Integration
git push origin main
```

Vergiss nicht die Environment Variables im Vercel Dashboard zu setzen!

## ğŸ›£ï¸ Roadmap

### Phase 1: Foundation (âœ… Erledigt)
- [x] Next.js Setup mit TypeScript & Tailwind
- [x] Supabase Integration
- [x] NLP Mock-Implementierung
- [x] API Route fÃ¼r LLM
- [x] Zen Editor Component

### Phase 2: Echte NLP Integration
- [ ] Transformers.js fÃ¼r Embeddings
- [ ] UDPipe WASM fÃ¼r Dependency Parsing
- [ ] Web Worker fÃ¼r Threading

### Phase 3: UI/UX
- [ ] Diff-Visualisierung (Phase 2 Feedback)
- [ ] Dashboard fÃ¼r SRS-Fortschritt
- [ ] Onboarding-Flow
- [ ] Auth (Email/Password + Anonym)

### Phase 4: Content & Production
- [ ] Erste 10-20 Source Texts laden
- [ ] Production Deployment
- [ ] Domain Setup

## ğŸ“š Technische Details

### Stilmetriken (Vektor B)
- **Dependency Distance**: Mittlere Distanz zwischen abhÃ¤ngigen WÃ¶rtern
- **Adjektiv/Verb-Ratio**: VerhÃ¤ltnis von Adjektiven zu Verben
- **SatzlÃ¤ngenvarianz**: Standardabweichung der Wortanzahl pro Satz

### Spaced Repetition (SM-2)
- **Fuzzy Grading**: Levenshtein-Distanz statt manueller Bewertung
- **Intervalle**: 1 Tag â†’ 6 Tage â†’ n Ã— EF-Faktor
- **Easiness Factor**: 1.3 - 2.5 (dynamisch angepasst)

### Rate Limiting
- **Authenticated Users**: 5 LLM-Calls/Tag
- **Anonymous Users**: 3 LLM-Calls/Tag (IP-basiert)
- **Reset**: TÃ¤glich um Mitternacht (UTC)

## ğŸ¤ Contributing

Aktuell ist dies ein privates Projekt. Bei Interesse an Zusammenarbeit bitte Kontakt aufnehmen.

## ğŸ“„ Lizenz

Noch nicht festgelegt.

## ğŸ™ Danksagungen

- AWS Bedrock fÃ¼r Claude 3.5 Haiku Zugang
- Anthropic fÃ¼r das Claude-Modell
- Vercel fÃ¼r generous Free Tier
- Supabase fÃ¼r PostgreSQL + pgvector
- Universal Dependencies fÃ¼r UDPipe

---

**Status**: ğŸš§ In Entwicklung - Phase 1 abgeschlossen
